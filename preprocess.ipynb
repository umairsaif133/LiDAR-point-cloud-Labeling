{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30efaebe-7673-445e-8dca-68350c9bc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 LAS files\n",
      "\n",
      "Processing: Site1_0.las\n",
      "  Loaded 1,917,694 points\n",
      "  Found 1 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 1,916,653 (99.95%)\n",
      "    Class 1 (Pothole): 1,041 (0.05%)\n",
      "\n",
      "Processing: Site1_1.las\n",
      "  Loaded 306,546 points\n",
      "  Found 20 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 298,388 (97.34%)\n",
      "    Class 1 (Pothole): 8,158 (2.66%)\n",
      "\n",
      "Processing: Site1_10.las\n",
      "  Loaded 497,676 points\n",
      "  Found 38 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 464,556 (93.35%)\n",
      "    Class 1 (Pothole): 33,120 (6.65%)\n",
      "\n",
      "Processing: Site1_11.las\n",
      "  Loaded 455,059 points\n",
      "  Found 30 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 429,402 (94.36%)\n",
      "    Class 1 (Pothole): 24,298 (5.34%)\n",
      "    Class 3 (Edge Crack): 1,359 (0.30%)\n",
      "\n",
      "Processing: Site1_12.las\n",
      "  Loaded 456,490 points\n",
      "  Found 35 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 425,765 (93.27%)\n",
      "    Class 1 (Pothole): 30,725 (6.73%)\n",
      "\n",
      "Processing: Site1_13.las\n",
      "  Loaded 698,678 points\n",
      "  Found 9 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 687,069 (98.34%)\n",
      "    Class 1 (Pothole): 11,609 (1.66%)\n",
      "\n",
      "Processing: Site1_14.las\n",
      "  Loaded 415,143 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 415,143 (100.00%)\n",
      "\n",
      "Processing: Site1_2.las\n",
      "  Loaded 356,774 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 356,774 (100.00%)\n",
      "\n",
      "Processing: Site1_3.las\n",
      "  Loaded 373,677 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 373,677 (100.00%)\n",
      "\n",
      "Processing: Site1_4.las\n",
      "  Loaded 336,539 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 336,539 (100.00%)\n",
      "\n",
      "Processing: Site1_5.las\n",
      "  Loaded 315,342 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 315,342 (100.00%)\n",
      "\n",
      "Processing: Site1_6.las\n",
      "  Loaded 254,686 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 254,686 (100.00%)\n",
      "\n",
      "Processing: Site1_7.las\n",
      "  Loaded 300,495 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 300,495 (100.00%)\n",
      "\n",
      "Processing: Site1_8.las\n",
      "  Loaded 438,942 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 438,942 (100.00%)\n",
      "\n",
      "Processing: Site1_9.las\n",
      "  Loaded 439,928 points\n",
      "  Found 0 damage regions\n",
      "  Label distribution:\n",
      "    Class 0 (Normal Road): 439,928 (100.00%)\n",
      "\n",
      "============================================================\n",
      "TOTAL STATISTICS\n",
      "============================================================\n",
      "Total damage patches: 163\n",
      "Total normal patches: 827\n",
      "Dataset balance:\n",
      "  Damage patches: 163\n",
      "  Normal patches: 244\n",
      "  Total patches: 407\n",
      "  Damage ratio: 40.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing patches: 100%|██████████| 325/325 [00:00<00:00, 688.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 325 patches to C:/Users/umair.muhammad/Documents/PhD/Research Work/FedLearn/training/All_Nome/h5\\train_balanced.h5\n",
      "\n",
      "Class distribution:\n",
      "  Normal Road: 2,568,810 points (96.48%)\n",
      "  Pothole: 92,456 points (3.47%)\n",
      "  Crack: 0 points (0.00%)\n",
      "  Edge Crack: 1,134 points (0.04%)\n",
      "  Flushed Out: 0 points (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing patches: 100%|██████████| 82/82 [00:00<00:00, 2557.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 82 patches to C:/Users/umair.muhammad/Documents/PhD/Research Work/FedLearn/training/All_Nome/h5\\val_balanced.h5\n",
      "\n",
      "Class distribution:\n",
      "  Normal Road: 649,591 points (96.70%)\n",
      "  Pothole: 22,153 points (3.30%)\n",
      "  Crack: 0 points (0.00%)\n",
      "  Edge Crack: 0 points (0.00%)\n",
      "  Flushed Out: 0 points (0.00%)\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS:\n",
      "============================================================\n",
      "1. Use the generated H5 files for training\n",
      "2. Apply class weights: {0: 1.0, 1: 20.0, 2: 150.0, 3: 500.0, 4: 50.0}\n",
      "3. Use Focal Loss for extreme imbalance\n",
      "4. Monitor per-class IoU, not just overall accuracy\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optimal Preprocessing Pipeline for Multi-Class Road Damage Segmentation\n",
    "Handles extreme class imbalance (95% normal, 5% damage across 4 classes)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import laspy\n",
    "import h5py\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Class definitions\n",
    "    'classes': {\n",
    "        0: 'Normal Road',\n",
    "        1: 'Pothole',\n",
    "        2: 'Crack',\n",
    "        3: 'Edge Crack',\n",
    "        4: 'Flushed Out'\n",
    "    },\n",
    "    \n",
    "    # Patch settings\n",
    "    'patch_size': 10.0,  # 10m x 10m patches (better for long roads)\n",
    "    'points_per_patch': 8192,  # Larger patches for better context\n",
    "    'stride': 5.0,  # 50% overlap\n",
    "    \n",
    "    # Sampling strategy\n",
    "    'damage_patch_ratio': 0.4,  # 40% of patches should contain damage\n",
    "    'min_damage_points': 50,  # Minimum damage points to consider patch valid\n",
    "    \n",
    "    # Data augmentation\n",
    "    'augmentation': {\n",
    "        'rotation': True,\n",
    "        'jitter': 0.01,\n",
    "        'scale': (0.9, 1.1),\n",
    "        'dropout': 0.05  # Random point dropout\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: PARSE BOUNDING BOXES\n",
    "# ============================================================================\n",
    "\n",
    "def parse_damage_bboxes(csv_file, margin=0.3):\n",
    "    \"\"\"\n",
    "    Parse CSV with S/F points and create bounding boxes for each damage type\n",
    "    \n",
    "    CSV format:\n",
    "    Name,X,Y,Z,Type\n",
    "    Site1_P1_S,x,y,z,Pothole\n",
    "    Site1_P1_F,x,y,z,Pothole\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract damage ID and type\n",
    "    df['Damage_ID'] = df['Name'].str.extract(r'_(P\\d+|C\\d+|E\\d+|F\\d+)')[0]\n",
    "    df['Point_Type'] = df['Name'].str.extract(r'_(S|F)$')[0]\n",
    "    \n",
    "    # Determine damage class from Name or Type column\n",
    "    def get_damage_class(row):\n",
    "        name = row['Name'].upper()\n",
    "        if 'P' in name or (pd.notna(row.get('Type')) and 'POTHOLE' in str(row['Type']).upper()):\n",
    "            return 1  # Pothole\n",
    "        elif 'C' in name and 'E' not in name or (pd.notna(row.get('Type')) and 'CRACK' in str(row['Type']).upper() and 'EDGE' not in str(row['Type']).upper()):\n",
    "            return 2  # Crack\n",
    "        elif 'E' in name or (pd.notna(row.get('Type')) and 'EDGE' in str(row['Type']).upper()):\n",
    "            return 3  # Edge Crack\n",
    "        elif 'F' in name or (pd.notna(row.get('Type')) and 'FLUSH' in str(row['Type']).upper()):\n",
    "            return 4  # Flushed Out\n",
    "        return 0\n",
    "    \n",
    "    df['Class'] = df.apply(get_damage_class, axis=1)\n",
    "    \n",
    "    bboxes = []\n",
    "    damage_ids = df['Damage_ID'].dropna().unique()\n",
    "    \n",
    "    for dmg_id in damage_ids:\n",
    "        dmg_df = df[df['Damage_ID'] == dmg_id]\n",
    "        start = dmg_df[dmg_df['Point_Type'] == 'S']\n",
    "        finish = dmg_df[dmg_df['Point_Type'] == 'F']\n",
    "        \n",
    "        if len(start) > 0 and len(finish) > 0:\n",
    "            x_s, y_s, z_s = start.iloc[0][['X', 'Y', 'Z']].values\n",
    "            x_f, y_f, z_f = finish.iloc[0][['X', 'Y', 'Z']].values\n",
    "            damage_class = start.iloc[0]['Class']\n",
    "            \n",
    "            bbox = {\n",
    "                'damage_id': dmg_id,\n",
    "                'class': damage_class,\n",
    "                'x_min': min(x_s, x_f) - margin,\n",
    "                'x_max': max(x_s, x_f) + margin,\n",
    "                'y_min': min(y_s, y_f) - margin,\n",
    "                'y_max': max(y_s, y_f) + margin,\n",
    "                'z_min': min(z_s, z_f) - margin,\n",
    "                'z_max': max(z_s, z_f) + margin,\n",
    "            }\n",
    "            bboxes.append(bbox)\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: LABEL POINTS WITH DAMAGE CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "def label_points_from_bboxes(points, bboxes):\n",
    "    \"\"\"Assign class labels to points based on bounding boxes\"\"\"\n",
    "    labels = np.zeros(len(points), dtype=np.int32)  # Default: normal road\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        mask = (\n",
    "            (points[:, 0] >= bbox['x_min']) & (points[:, 0] <= bbox['x_max']) &\n",
    "            (points[:, 1] >= bbox['y_min']) & (points[:, 1] <= bbox['y_max']) &\n",
    "            (points[:, 2] >= bbox['z_min']) & (points[:, 2] <= bbox['z_max'])\n",
    "        )\n",
    "        labels[mask] = bbox['class']\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: REFINE LABELS WITH DBSCAN\n",
    "# ============================================================================\n",
    "\n",
    "def refine_labels_with_clustering(points, labels, eps=0.5, min_samples=10):\n",
    "    \"\"\"Use DBSCAN to refine damage labels and remove outliers\"\"\"\n",
    "    refined_labels = labels.copy()\n",
    "    \n",
    "    for damage_class in [1, 2, 3, 4]:\n",
    "        damage_mask = labels == damage_class\n",
    "        if np.sum(damage_mask) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        damage_points = points[damage_mask]\n",
    "        \n",
    "        # Cluster damage points\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(damage_points)\n",
    "        \n",
    "        # Remove noise points (label -1)\n",
    "        noise_mask = clustering.labels_ == -1\n",
    "        damage_indices = np.where(damage_mask)[0]\n",
    "        noise_indices = damage_indices[noise_mask]\n",
    "        refined_labels[noise_indices] = 0  # Set to normal\n",
    "    \n",
    "    return refined_labels\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: INTELLIGENT PATCH EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_balanced_patches(points, labels, config):\n",
    "    \"\"\"\n",
    "    Extract patches with balanced sampling:\n",
    "    - Ensure adequate representation of damage classes\n",
    "    - Use sliding window for full coverage\n",
    "    - Oversample damage-containing patches\n",
    "    \"\"\"\n",
    "    patch_size = config['patch_size']\n",
    "    stride = config['stride']\n",
    "    points_per_patch = config['points_per_patch']\n",
    "    \n",
    "    x_min, y_min = points[:, 0].min(), points[:, 1].min()\n",
    "    x_max, y_max = points[:, 0].max(), points[:, 1].max()\n",
    "    \n",
    "    damage_patches = []\n",
    "    normal_patches = []\n",
    "    \n",
    "    # Sliding window extraction\n",
    "    x = x_min\n",
    "    while x < x_max:\n",
    "        y = y_min\n",
    "        while y < y_max:\n",
    "            # Define patch bounds\n",
    "            mask = (\n",
    "                (points[:, 0] >= x) & (points[:, 0] < x + patch_size) &\n",
    "                (points[:, 1] >= y) & (points[:, 1] < y + patch_size)\n",
    "            )\n",
    "            \n",
    "            patch_indices = np.where(mask)[0]\n",
    "            \n",
    "            if len(patch_indices) < 500:  # Skip sparse patches\n",
    "                y += stride\n",
    "                continue\n",
    "            \n",
    "            patch_points = points[patch_indices]\n",
    "            patch_labels = labels[patch_indices]\n",
    "            \n",
    "            # Count damage points\n",
    "            damage_count = np.sum(patch_labels > 0)\n",
    "            damage_ratio = damage_count / len(patch_labels)\n",
    "            \n",
    "            # Sample points\n",
    "            if len(patch_points) >= points_per_patch:\n",
    "                choice = np.random.choice(len(patch_points), points_per_patch, replace=False)\n",
    "            else:\n",
    "                choice = np.random.choice(len(patch_points), points_per_patch, replace=True)\n",
    "            \n",
    "            sampled_points = patch_points[choice]\n",
    "            sampled_labels = patch_labels[choice]\n",
    "            \n",
    "            # Categorize patch\n",
    "            if damage_count >= config['min_damage_points']:\n",
    "                damage_patches.append({\n",
    "                    'points': sampled_points,\n",
    "                    'labels': sampled_labels,\n",
    "                    'damage_ratio': damage_ratio,\n",
    "                    'class_distribution': np.bincount(sampled_labels, minlength=5)\n",
    "                })\n",
    "            else:\n",
    "                normal_patches.append({\n",
    "                    'points': sampled_points,\n",
    "                    'labels': sampled_labels\n",
    "                })\n",
    "            \n",
    "            y += stride\n",
    "        x += stride\n",
    "    \n",
    "    return damage_patches, normal_patches\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: BALANCE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def create_balanced_dataset(damage_patches, normal_patches, config):\n",
    "    \"\"\"\n",
    "    Create balanced training dataset:\n",
    "    - Keep all damage patches (minority class)\n",
    "    - Undersample normal patches\n",
    "    - Add extra copies of rare damage types\n",
    "    \"\"\"\n",
    "    num_damage = len(damage_patches)\n",
    "    damage_ratio = config['damage_patch_ratio']\n",
    "    \n",
    "    # Calculate how many normal patches to keep\n",
    "    num_normal_target = int(num_damage * (1 - damage_ratio) / damage_ratio)\n",
    "    \n",
    "    # Randomly sample normal patches\n",
    "    if len(normal_patches) > num_normal_target:\n",
    "        normal_indices = np.random.choice(len(normal_patches), num_normal_target, replace=False)\n",
    "        selected_normal = [normal_patches[i] for i in normal_indices]\n",
    "    else:\n",
    "        selected_normal = normal_patches\n",
    "    \n",
    "    # Oversample rare damage types\n",
    "    class_counts = {}\n",
    "    for patch in damage_patches:\n",
    "        main_class = np.argmax(patch['class_distribution'][1:]) + 1  # Ignore class 0\n",
    "        class_counts[main_class] = class_counts.get(main_class, 0) + 1\n",
    "    \n",
    "    # Find rarest class\n",
    "    if class_counts:\n",
    "        max_count = max(class_counts.values())\n",
    "        oversampled_damage = damage_patches.copy()\n",
    "        \n",
    "        for damage_class, count in class_counts.items():\n",
    "            if count < max_count * 0.5:  # If less than 50% of most common\n",
    "                # Find patches of this class\n",
    "                class_patches = [p for p in damage_patches \n",
    "                               if np.argmax(p['class_distribution'][1:]) + 1 == damage_class]\n",
    "                # Duplicate them\n",
    "                oversample_times = int(max_count / count) - 1\n",
    "                oversampled_damage.extend(class_patches * oversample_times)\n",
    "    else:\n",
    "        oversampled_damage = damage_patches\n",
    "    \n",
    "    # Combine\n",
    "    all_patches = oversampled_damage + selected_normal\n",
    "    np.random.shuffle(all_patches)\n",
    "    \n",
    "    print(f\"Dataset balance:\")\n",
    "    print(f\"  Damage patches: {len(oversampled_damage)}\")\n",
    "    print(f\"  Normal patches: {len(selected_normal)}\")\n",
    "    print(f\"  Total patches: {len(all_patches)}\")\n",
    "    print(f\"  Damage ratio: {len(oversampled_damage)/len(all_patches)*100:.1f}%\")\n",
    "    \n",
    "    return all_patches\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: NORMALIZATION & AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def normalize_patch(points):\n",
    "    \"\"\"Normalize patch to unit sphere\"\"\"\n",
    "    centroid = np.mean(points[:, :3], axis=0)\n",
    "    points[:, :3] -= centroid\n",
    "    max_dist = np.max(np.sqrt(np.sum(points[:, :3]**2, axis=1)))\n",
    "    if max_dist > 0:\n",
    "        points[:, :3] /= max_dist\n",
    "    return points\n",
    "\n",
    "def augment_patch(points, labels, config):\n",
    "    \"\"\"Apply data augmentation\"\"\"\n",
    "    aug_config = config['augmentation']\n",
    "    \n",
    "    # Random rotation around Z-axis\n",
    "    if aug_config['rotation']:\n",
    "        theta = np.random.uniform(0, 2 * np.pi)\n",
    "        cos_t, sin_t = np.cos(theta), np.sin(theta)\n",
    "        rotation = np.array([[cos_t, -sin_t, 0],\n",
    "                           [sin_t, cos_t, 0],\n",
    "                           [0, 0, 1]])\n",
    "        points[:, :3] = points[:, :3] @ rotation.T\n",
    "    \n",
    "    # Random jitter\n",
    "    points[:, :3] += np.random.normal(0, aug_config['jitter'], points[:, :3].shape)\n",
    "    \n",
    "    # Random scaling\n",
    "    scale = np.random.uniform(*aug_config['scale'])\n",
    "    points[:, :3] *= scale\n",
    "    \n",
    "    # Random point dropout\n",
    "    if aug_config['dropout'] > 0:\n",
    "        keep_mask = np.random.random(len(points)) > aug_config['dropout']\n",
    "        keep_indices = np.where(keep_mask)[0]\n",
    "        if len(keep_indices) >= len(points) // 2:  # Keep at least 50%\n",
    "            points = points[keep_indices]\n",
    "            labels = labels[keep_indices]\n",
    "            # Resample to original size\n",
    "            resample_indices = np.random.choice(len(points), CONFIG['points_per_patch'], replace=True)\n",
    "            points = points[resample_indices]\n",
    "            labels = labels[resample_indices]\n",
    "    \n",
    "    return points, labels\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: SAVE TO H5 FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def save_patches_to_h5(patches, output_file, config, augment=False):\n",
    "    \"\"\"Save patches to H5 format for efficient loading\"\"\"\n",
    "    all_points = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for patch in tqdm(patches, desc=\"Processing patches\"):\n",
    "        points = patch['points'].copy()\n",
    "        labels = patch['labels'].copy()\n",
    "        \n",
    "        # Normalize\n",
    "        points = normalize_patch(points)\n",
    "        \n",
    "        # Augment if training\n",
    "        if augment:\n",
    "            points, labels = augment_patch(points, labels, config)\n",
    "        \n",
    "        all_points.append(points)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_points = np.array(all_points, dtype=np.float32)\n",
    "    all_labels = np.array(all_labels, dtype=np.int32)\n",
    "    \n",
    "    # Save to H5\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        f.create_dataset('data', data=all_points, compression='gzip')\n",
    "        f.create_dataset('label', data=all_labels, compression='gzip')\n",
    "        f.create_dataset('num_classes', data=5)\n",
    "        \n",
    "        # Save class statistics\n",
    "        class_counts = np.bincount(all_labels.flatten(), minlength=5)\n",
    "        f.create_dataset('class_counts', data=class_counts)\n",
    "        \n",
    "        # Save config\n",
    "        f.attrs['patch_size'] = config['patch_size']\n",
    "        f.attrs['points_per_patch'] = config['points_per_patch']\n",
    "    \n",
    "    print(f\"\\nSaved {len(patches)} patches to {output_file}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    class_counts = np.bincount(all_labels.flatten(), minlength=5)\n",
    "    for i, (class_name, count) in enumerate(zip(config['classes'].values(), class_counts)):\n",
    "        percentage = 100 * count / class_counts.sum()\n",
    "        print(f\"  {class_name}: {count:,} points ({percentage:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_single_file(las_file, csv_file, config):\n",
    "    \"\"\"Preprocess a single LAS file with its labels\"\"\"\n",
    "    print(f\"\\nProcessing: {os.path.basename(las_file)}\")\n",
    "    \n",
    "    # Load LAS\n",
    "    las = laspy.read(las_file)\n",
    "    points = np.vstack((las.x, las.y, las.z)).T\n",
    "    \n",
    "    # Add intensity if available\n",
    "    if hasattr(las, 'intensity'):\n",
    "        intensity = las.intensity.reshape(-1, 1) / 255.0\n",
    "        points = np.hstack((points, intensity))\n",
    "    \n",
    "    print(f\"  Loaded {len(points):,} points\")\n",
    "    \n",
    "    # Parse bounding boxes\n",
    "    bboxes = parse_damage_bboxes(csv_file)\n",
    "    print(f\"  Found {len(bboxes)} damage regions\")\n",
    "    \n",
    "    # Label points\n",
    "    labels = label_points_from_bboxes(points, bboxes)\n",
    "    \n",
    "    # Refine with clustering\n",
    "    labels = refine_labels_with_clustering(points, labels)\n",
    "    \n",
    "    # Print label statistics\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Label distribution:\")\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"    Class {cls} ({config['classes'][cls]}): {count:,} ({100*count/len(labels):.2f}%)\")\n",
    "    \n",
    "    # Extract patches\n",
    "    damage_patches, normal_patches = extract_balanced_patches(points, labels, config)\n",
    "    \n",
    "    return damage_patches, normal_patches\n",
    "\n",
    "def preprocess_all_files(las_folder, csv_folder, output_folder, config):\n",
    "    \"\"\"Preprocess all LAS files\"\"\"\n",
    "    import glob\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    las_files = sorted(glob.glob(os.path.join(las_folder, '*.las')))\n",
    "    print(f\"Found {len(las_files)} LAS files\")\n",
    "    \n",
    "    all_damage_patches = []\n",
    "    all_normal_patches = []\n",
    "    \n",
    "    for las_file in las_files:\n",
    "        base_name = os.path.splitext(os.path.basename(las_file))[0]\n",
    "        csv_file = os.path.join(csv_folder, base_name + '.csv')\n",
    "        \n",
    "        if not os.path.exists(csv_file):\n",
    "            print(f\"Warning: No CSV found for {base_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            damage_patches, normal_patches = preprocess_single_file(las_file, csv_file, config)\n",
    "            all_damage_patches.extend(damage_patches)\n",
    "            all_normal_patches.extend(normal_patches)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {base_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOTAL STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total damage patches: {len(all_damage_patches)}\")\n",
    "    print(f\"Total normal patches: {len(all_normal_patches)}\")\n",
    "    \n",
    "    # Create balanced dataset\n",
    "    balanced_patches = create_balanced_dataset(all_damage_patches, all_normal_patches, config)\n",
    "    \n",
    "    # Split train/val\n",
    "    np.random.shuffle(balanced_patches)\n",
    "    split_idx = int(0.8 * len(balanced_patches))\n",
    "    train_patches = balanced_patches[:split_idx]\n",
    "    val_patches = balanced_patches[split_idx:]\n",
    "    \n",
    "    # Save\n",
    "    save_patches_to_h5(train_patches, \n",
    "                      os.path.join(output_folder, 'train_balanced.h5'),\n",
    "                      config, augment=True)\n",
    "    save_patches_to_h5(val_patches,\n",
    "                      os.path.join(output_folder, 'val_balanced.h5'),\n",
    "                      config, augment=False)\n",
    "    \n",
    "    print(f\"\\n✓ Preprocessing complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure paths\n",
    "    LAS_FOLDER = \"C:/Users/umair.muhammad/Documents/PhD/Research Work/FedLearn/training/All_Nome/LAS_Files_Site1\"\n",
    "    CSV_FOLDER = \"C:/Users/umair.muhammad/Documents/PhD/Research Work/FedLearn/training/All_Nome/Labels_Site1\"\n",
    "    OUTPUT_FOLDER = \"C:/Users/umair.muhammad/Documents/PhD/Research Work/FedLearn/training/All_Nome/h5\"\n",
    "    \n",
    "    # Run preprocessing\n",
    "    preprocess_all_files(LAS_FOLDER, CSV_FOLDER, OUTPUT_FOLDER, CONFIG)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. Use the generated H5 files for training\")\n",
    "    print(\"2. Apply class weights: {0: 1.0, 1: 20.0, 2: 150.0, 3: 500.0, 4: 50.0}\")\n",
    "    print(\"3. Use Focal Loss for extreme imbalance\")\n",
    "    print(\"4. Monitor per-class IoU, not just overall accuracy\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db48915-dbf7-4f14-8f53-6f1646317fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
